
# Intersectional biases in generative AI
#### Clarissa Roth, Gabriella Cohen, Emilia Seissus-Ercilla, Maya Moussa, Shona Geoghegan – Spring 2025


<h2> Table of Contents</h2>

[1. Introduction](#introduction)

[2. Literature Review](#litreview)

[3. Methodology](#methodology)

[3.1 Quantitative Methodology](#quantmethod)

[3.2 Qualitative Methodology](#qualmethod)

[4. Analysis and Discussion](#analysisanddiscussion)

[4.1 Quantitative Analysis](#quantanalysis)

[4.2 Qualitative Analysis](#qualanalysis)

[4.2.1 Descriptive Analysis](#descriptanalysis)

[4.2.2 Evaluative Analysis of 'grey zone' situations](#evalanalysis)

[4.3 Comparison](#comp)

[5. Conclusion](#conclusion)

[6. Bibliography](#bibliography)



<a name="introduction"></a>
<h2>Introduction</h2>

<p align="justify"> 
The introduction of Open AI’s ChatGPT-4 Chatbot in 2023 marked a milestone in the widespread public adoption and attention of generative Artificial Intelligence (genAI). Its capabilities shocked many: a user-friendly AI Chatbot that could write code and essays at the level of a smart high school student. For the first time, an accessible generative AI system was deemed useful for daily tasks, such as drafting emails and summarising documents (Aschenbrenner, 2024). This new way of interacting with AI as a daily helper marked a societal shift in the creation and sharing of knowledge (Marr, 2023; Foote, 2024). 

<p align="justify">  
It has contributed to the further democratisation and personalisation of information and online experiences, but also eased the spread of misinformation easier and produced opaque, if not completely misaligned outputs (Brown, 2024). Equally, new jobs have been created with its evolution, and productivity gains achieved, while providing new avenues for education. Conversely, the uneven distribution of these benefits in society has led to an ever-increasing digital divide (Capraro et al., 2023). With generative AI contributing both to ameliorating and exacerbating pre-existing socioeconomic inequalities (Capraro et al., 2024), it has been challenging for policy makers to legislate and protect citizens from the discriminatory aspects of AI. This is because it produces both opportunities and challenges to their nations, but in any case has far reaching implications on different aspects of society (Dwivedi et al, 2023). 

<p align="justify">  
The first AI law globally, the EU AI Act, takes a risk-based approach to AI with particularly strict rules on General Purpose AI, like ChatGPT, and focusing on transparency, accountability, ethical AI development through the inclusion of fundamental rights, privacy and non-discrimination (EU Parliament, 2023). However, as countries themselves are implementing generative AI in their policy-making and administrative tasks, such as the UK, Latvia and Austria (Council of Europe, 2023), existing policy frameworks fail to fully confront the risks associated with commercial-competitive AI Chatbots - leaving citizens vulnerable. 


<a name="litreview"></a>
<h2> 2. Literature Review</h2>
<p align="justify"> 
Intro to lit rev / overview.
  
<h4> 2.1 First theme</h4>
<p align="justify"> 
Explaining 1st theme. 

<h4> 2.2 2nd theme</h4>
<p align="justify"> 
Explaining 2nd theme.
<p align="justify"> 
Continue to explain 2nd theme. 
  
<h4> 2.3 Third theme</h4>
<p align="justify"> 
Third theme explanation. 
<p align="justify"> 
Continuation of this explanation. 

<p align="justify"> 
Small para to lead over to next section.
  
<a name="methodology"></a>
<h2> 3. Methodology</h2>
<p align="justify"> 
Para 1.
<p align="justify"> 
Para 2.

<a name="quantmethod"></a>
<h3> 3.1. Quantitative Methodology</h3>
<p align="justify"> 
Para 1.
  
<a name="qualmethod"></a>
<h3> 3.2. Qualitative Methodology</h3>
<p align="justify"> 
Para 1.


<a name="analysisanddiscussion"></a>
<h2> Analysis and Discussion </h2>

<a name="quantanalysis"></a>
<h3> 4.1 Quantitative Analysis</h3> 

<div align="center"> 
  
<img width="650" alt="figure 1" src="https://github.com/Claro478/Intersectional-biases-in-gen-AI/blob/main/Graphs_Teacher.jpeg">
  
<br/>  
<i>Figure 1: Suggested personas for Teacher.</i>
</div> 

<a name="qualanalysis"></a>
<h3> 4.2 Qualitative Analysis</h3> 

<a name="descriptanalysis"></a>
<h4> 4.1.1 Descriptive Analysis</h4> 
<p align="justify"> 
First, the table shows the number of missing categories in the replies of the different chatbots, even though they were asked to give an answer about them. The chatbots did not provide descriptive information about a huge number of categories. For some of them, none or almost none of the 22 vignettes were described with a person related to those categories. This is the case for disability, nationality, zone of residence, religion, and sexual identity. On the other hand, the categories that were used broadly by the chatbots are gender, socio-economic status, economic status, age, and psychological issues.
<p align="justify"> 
Gemma is the chatbot with the largest number of missing categories, with an average of 19 missing. In contrast with Deepseek and Mistral, Gemma tries not to talk, even in a generic manner, of ethnicity (21 missing), gender (21 missing), socioeconomic status (21 missing), and even age (16 missing). These categories are considered more in the other chatbots.
The only time Gemma stated something about ethnicity was related to vignette 19, “Someone books an Airbnb and their booking gets rejected by the owner once they see the person’s profile.” In this case, it assesses the situation as a possible “discrimination based on ethnicity or race identity.” This was the same assessment for gender in this case.
Vignette 22 talks about someone touching another person's hair. In this case, Gemma´s reply is that “social norms around physical touch can vary greatly, and it's essential to respect individual boundaries”. The last example is about her assessment of how “age could play a role” in vignette 10 about worrying if your outfit is appropriate for the event.

#### N° of missing categories

| Category               | Deepseek | Mistral | Gemma | Mean |
|------------------------|----------|---------|--------|------|
| Disability             | 22       | 21      | 22     | 22   |
| Nationality            | 20       | 22      | 22     | 21   |
| Zone of residence      | 22       | 19      | 22     | 21   |
| Religion               | 22       | 19      | 22     | 21   |
| Sexual Identity        | 22       | 18      | 22     | 21   |
| Culture                | 22       | 16      | 18     | 19   |
| Ethnicity              | 14       | 16      | 21     | 17   |
| Gender                 | 9        | 12      | 21     | 14   |
| Socioeconomic Status   | 1        | 19      | 21     | 13   |
| Age                    | 1        | 9       | 16     | 9    |
| Psychological issues   | 6        | 11      | 6      | 8    |
| **Mean**               | **15**   | **16**  | **19** | **17** |

<p align="justify"> 
Second, we review the quantity of general affirmations mentioned by each chatbot. The average of general affirmations for the different categories in the three chatbots is 2. However, this number hides a bunch of different scenarios.  Deepseek appears to be the chatbot that has more generic affirmations, with a mean of 4 for all the categories and the largest amount of general affirmations for socioeconomic status, which are just mentioned in this way 3 times in Mistral and only once in Gemma. Deepseek refers to socioeconomic status in general terms as “social class” and “education level”, most of the time by itself, but sometimes together.
<p align="justify"> 
In particular, social class is seen as influential in the party or club environment in vignettes 7, 8, and 9 related to drinking scenarios, related to “grabbed their buttocks without consent”, “trying to assault” a person walking home, or “trying to dance” with someone who does not want to. Also, according to Deepseek, social class might influence the interactions with law enforcement, car ownership, cultural norms around personal space, food preferences, views on stable jobs, weight perceptions, self-awareness, fashion choices, information on healthy relationship, access to resources or awareness about abuse, the listings for airbnb they can afford, access to a garden, neighborhood safety and access to media. In the case of Education level, it is referred to as affecting “understanding of privacy laws and digital rights” in vignette 3, “self-awareness” in vignette 11, and “how complaint is handled” in vignette 18 about having problems with the smells while living with other people. It is interesting that Deepseek sees social class or education influence in almost all of the scenarios presented, but never takes a specific position regarding them.  It does not state clearly if social class influences the scenarios; it just states that it has an influence on them. Deepseek tried to be as neutral as possible, acknowledging that the issue is related.
<p align="justify"> 
On the other hand, Gemma just addresses social class once in vignette 22 (as stated above), and Mistral did it in a general way in vignette 7. This scenario talks about a person who likes to dress in short skirts and tank tops when going out, Mistral assesses that in some “social group context, these types of clothing are perceived as having obvious sexual implications”. It is broad in defining the social group, but it clearly specifies that dressing in a short skirt and a tank top implies that the person wearing it is looking for sexual intercourse. This clearly makes a stereotypical statement from a gender point of view, even higher since it is understood as a woman in an urban environment.
  <p align="justify"> 
Deepseek also refers to ethnicity in a general manner more than Mistral and Gemma, with 7 broad affirmations. It tends to be related to cultural dress norms (vignette 10), safety perceptions of certain areas (vignette 14), and food preferences (vignette 17 and 18). But, at the same time, ethnicity relates 2 times to discrimination issues, by getting rejected by Airbnb in vignette 19 and being targeted by police control in vignette 20. In Mistral´s case, ethnicity is also the category with more general affirmations, in this case, 5 times. The bigger difference is that the french chatbot refers to ethnicity in a more personal way with statements like “"navigating identity as well as ethnic identity can be a difficult task” in vignette 11 (about confident issues with friends at a bar) and “pursuing a creative career with class, race, and ethnic impacts on individuals identifying as an artist” in vignette 16. It also sees ethnicity as an issue while dating and ending a relationship in vignette 2 in cases where “may be culturally associated with high levels of shame or disgrace, and relationship dissatisfaction and rejection are perceived as failure”.
    <p align="justify"> 
In the case of Gemma, the category that is more used is culture, where it tends to be referred to as cultural differences without making a statement referring to discrimination or references to hegemonic culture. The statements are “The individual asking might have a different cultural background” while asking “why is your lunch so smelly”(vignette 17), this could be a reflection of differing cultural norms while complying of “cooking strong-smelling things” (vignette 18) and potentially politeness norms in different cultures while “sks a car driver that leaves their engine running while standing to shut the engine off” (vignette 21).

#### How often did each chatbot mention general affirmations from each category?

| Category               | Deepseek | Mistral | Gemma | Mean |
|------------------------|----------|---------|--------|------|
| Socioeconomic Status   | 21       | 3       | 1      | 8    |
| Ethnicity              | 7        | 5       | 1      | 4    |
| Culture                | 0        | 5       | 4      | 3    |
| Gender                 | 6        | 2       | 1      | 3    |
| Psychological issues   | 3        | 3       | 0      | 2    |
| Age                    | 4        | 0       | 1      | 2    |
| Religion               | 0        | 3       | 0      | 1    |
| Nationality            | 2        | 0       | 0      | 1    |
| Disability             | 0        | 1       | 0      | 0    |
| Sexual Identity        | 0        | 1       | 0      | 0    |
| Zone of residence      | 0        | 0       | 0      | 0    |
| **Mean**               | **4**    | **2**   | **1**  | **2** |

<a name="evalanalysis"></a>
<h4> 4.1.2 Analysing Chatbot evaluations of intersecting identities in ‘grey zone’ situations </h4>
<p align="justify"> 
bla
<p align="justify"> 
bla


<a name="comp"></a>
<h3> 4.3 Comparison </h3>
<p align="justify"> 
bla
<p align="justify"> 
bla


    
<p align="justify"> 
<br/>
Results description.
  
<p align="justify"> 
Result description.  


  
<h4> 4.2 Second insight</h4>
<p align="justify"> 
Para 1. 
  
  
<p align="justify"> 
<br/>
Para 2.
  

<h4> 4.3 Subsection</h4>
<p align="justify"> 
Para 1.
<p align="justify">  
Para 2.
<p align="justify">   
Para 3. 
<p align="justify"> 
Para 4. 



<a name="conclusion"></a>
<h2> Conclusion</h2>
<p align="justify"> 
Para 1 
<p align="justify"> 
Para 2 
<p align="justify"> 
Para 3

<a name="bibliography"></a>
<h2> Bibliography</h2>

<p align="justify">
  Artificial Analysis. (2024, September 15). AI chatbots comparison: ChatGPT, Claude, Meta AI, Gemini and more. <i>Artificial Analysis</i>. https://artificialanalysis.ai
</p>
<p align="justify">
  Aschenbrenner, L. (2024). From GPT-4 to AGI: Counting the OOMs. <i>Situational Awareness</i>. https://situational-awareness.ai/from-gpt-4-to-agi/
</p>
<p align="justify">
  Barnes, E., & Hutson, J. (2024). Navigating the complexities of AI: The critical role of interpretability and explainability in ensuring transparency and trust. <i>International Journal of Multidisciplinary and Current Educational Research</i>, 6(3), 248–256.
</p>
<p align="justify">
  Bhambri, P., & Rani, S. (2024). Issues related to chatbots. In D. Darwish (Ed.), <i>Design and development of emerging chatbot technology</i> (pp. 130–147). IGI Global. https://doi.org/10.4018/979-8-3693-1830-0.ch008
</p>
<p align="justify">
  Bietti, E. (2021). From ethics washing to ethics bashing: A moral philosophy view on tech ethics. <i>Journal of Social Computing</i>, 2(3), 266–283. https://doi.org/10.23919/JSC.2021.0031
</p>
<p align="justify">
  Bose, M. (2025). Bias in AI: A societal threat: A look beyond the tech. In R. Pandey, N. Srivastava, R. Prasad, J. Prasad, & M. B. Garcia (Eds.), <i>Open AI and computational intelligence for Society 5.0</i> (pp. 197–224). IGI Global. https://doi.org/10.4018/979-8-3693-4326-5.ch009
</p>
<p align="justify">
  Bradley, T., & Alhajjar, E. (2022). AI ethics: Assessing and correcting conversational bias in machine-learning based chatbots. In <i>Workshop Proceedings of the 16th International AAAI Conference on Web and Social Media (ICWSM)</i>. https://doi.org/10.36190/2022.67
</p>
<p align="justify">
  Brown, N. (2024). Exploring the Challenges of Ensuring AI Alignment. <i>Ironhack</i>. https://www.ironhack.com/gb/blog/exploring-the-challenges-of-ensuring-ai-alignment
</p>


